NAME: Saurav Rohira
EMAIL: saurav.rohira1999@gmail.com
ID: 505000422


2.1.1:
The amount of time it takes to create a pthread is a constant. For a high number of iterations the time taken to process a thread is able to exceed this time taken by to create additional threads. When two threads call the add function at once, which would thus happen only when iterations are high, an error appears. This is also why significantly smaller iterations rarely fail.

2.1.2:
Yield runs are significantly slower because a large amount of time is spent in performing context switches. It isn't possible to measure per-operation time, however, as it is impossible to isolate context switch time from add time.

2.1.3:
When measuring the time taken by the process, it includes pthread creation time + execution time. As iterations increase the creation time remains constant, therefore average time reduces. The correct cost can only be calculated with a theoretical infinite number of iterations, as this would make the overhead of pthread creation negligible and thus 0% of the avg time.

2.1.4:
These options only cause time to be spent in "blocking" when two threads are simultaneously trying to modify a shared variable, i.e. during a critical section. As discussed in 2.1.1, the probability of this happening is very low for a small number of threads. Therefore, when number of threads increases, there is a high cost in time for the time the various threads spend blocking each other.

2.2.1:
For both the list and add, the cost per operation seems to increase linearly. However the shape for the adds and lists varies slightly. For the adds the graphs at first rises more sharply, and then becomes flatter. Whereas, for the list the slop remains about constant throughout. This makes sense as for the add, with more threads, there is contention for the mutex that plays a role in addition to the locking and releasing of the mutex. However, for the list, the operations are far more computationally intense, and the overhead of the operations far exceeds the contention for the mutex.

2.2.2:
Spin locks appear to be far less scalable for a large number of threads than mutexes. Spin locks waste a lot more cpu time when 'spinning' and waiting for the lock. For a larger number of threads this impacts the cost far more than mutexes. And therefore, the curve for the spin lock, while the same as the mutex for the add and list graphs, has a far greater slope at every point (of course except 1 thread). In an environment with a very large number of threads, and high contention, this would mean than spin locks would waste a very large amount of time compared to the mutexes.