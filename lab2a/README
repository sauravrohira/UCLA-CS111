NAME: Saurav Rohira
EMAIL: saurav.rohira1999@gmail.com
ID: 505000422


2.1.1:
The amount of time it takes to create a pthread is a constant. For a high number of iterations the time taken to process a thread is able to exceed this time taken by to create additional threads. When two threads call the add function at once, which would thus happen only when iterations are high, an error appears. This is also why significantly smaller iterations rarely fail.

2.1.2:
Yield runs are significantly slower because a large amount of time is spent in performing context switches between processes, an operation that is very costly to performance. It isn't possible to measure per-operation time, however, as it is impossible to isolate time spent when performing a context switch from time spent performing the add operation.

2.1.3:
When measuring the total time taken by the process, it includes pthread creation time + execution time. As iterations increase execution time increases proportionally to the number of operations, but the pthread creation time remains the same. Therefore, this same time spent setting up the threads is divided up between a larger number of iterations. Therefore the average cost of each operation reduces. The correct cost can only be calculated with a theoretical infinite number of iterations, as this would make the overhead of pthread creation negligible and thus 0% of the avg time. This could be approximated at best to some degree of error with a very high number of iterations.

2.1.4:
The synchronization mechanisms only spend time "blocking," or making another thread wait, when two threads are simultaneously trying to modify a shared variable, i.e. when simultaneously accessing the same critical section. As discussed in 2.1.1, the probability of this happening is very low for a small number of threads. Therefore, when number of threads increases, there is a high cost generated various threads waiting as they block each other.

2.2.1:
For both the list and add, the cost per operation seems to increase linearly. However the shape for the adds and lists varies slightly. For the adds the graphs at first rises more sharply, and then becomes flatter. Whereas, for the list the slop remains about constant throughout. This makes sense as for the add, with more threads, there is contention for the mutex that plays a role in addition to the locking and releasing of the mutex. However, for the list, the operations are far more computationally intense, and the overhead of the operations far exceeds the contention for the mutex.

2.2.2:
Spin locks appear to be far less scalable for a large number of threads than mutexes. Spin locks waste a lot more cpu time when 'spinning' and waiting for the lock. For a larger number of threads this impacts the cost far more than mutexes. And therefore, the curve for the spin lock, while the same as the mutex for the add and list graphs, has a far greater slope at every point (of course except 1 thread). In an environment with a very large number of threads, and high contention, this would mean than spin locks would waste a very large amount of time compared to the mutexes.